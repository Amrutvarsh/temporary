{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amrutvarsh/temporary/blob/main/PDF_READER_RAGMODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee56d005"
      },
      "source": [
        "I've added a cell to install the necessary libraries. Please run it and then try running the code cell again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-core langchain-community pypdf pymupdf sentence-transformers faiss-cpu chromadb python-dotenv -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BJIYemp4Gc4j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "PzpCzYfBBM_J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Process_all_pdfs(pdf_directory):\n",
        "    all_documents=[]\n",
        "\n",
        "    pdf_dir=Path(pdf_directory)\n",
        "    pdf_files=[str(f) for f in pdf_dir.glob(\"**/*.pdf\")]\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            print(type(pdf_file))\n",
        "            loader = PyMuPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()\n",
        "\n",
        "            for doc in documents:\n",
        "                doc.metadata['source_file'] = str(pdf_file)\n",
        "                doc.metadata['file_type'] = 'pdf'\n",
        "\n",
        "            all_documents.extend(documents)\n",
        "        except Exception as e:\n",
        "            print(f\" Error:{e}\")\n",
        "\n",
        "    return all_documents\n",
        "\n",
        "all_pdfs=Process_all_pdfs(\"../content/sample_data/pdf_files\")\n",
        "\n",
        "all_pdfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HWWRbUepv-DI",
        "outputId": "994bc6bc-c1c6-4ce5-8fcb-d7e880d22685"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 PDF files to process\n",
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 0, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilities.\\nBuilt upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁed multi-task frame-\\nwork comprising specialized data transformation and training strategies.\\nThe\\ndata transformation scheme enables the incorporation of more diverse textual\\ntraining datasets, while the task-speciﬁc training strategies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline leveraging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentation, and Hard negative\\nexample generation to improve the semantic richness and sample diﬃculty of\\nthe training set. Additionally, we employ a two-stage training strategy, compris-\\ning initial retrieval-focused pretraining followed by full-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robust retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more diverse data is crucial for\\nadvancing retrieval model performance, and that leveraging LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructions on GitHub2.\\n1\\nIntroduction\\nText embedding models, which transform natural language text into mathematical vec-\\ntor representations, play an indispensable role in text mining, question-answering sys-\\ntems, recommendation systems, and retrieval-augmented generation. Recently, LLM-\\nbased agent technology has experienced rapid development and widespread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhanced agent systems\\n∗https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nin terms of real-time performance, long-term memory, data privacy preservation, and\\nknowledge integration capabilities. With the continuous advancement of neural net-\\nworks and deep learning, text embeddings have evolved from early sparse representa-\\ntions (e.g., BM25[1]) to dense representations based on ﬁne-tuned deep networks such\\nas BERT[2] and T5[3], leading to signiﬁcant performance improvements[4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatGPT[9], ushered in\\na new era of text embeddings based on LLM representations, including models like text-\\nembedding-3-large and RepLLaMA[10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For instance, to address\\nthe limitation of decoder-only architectures—where causal attention mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—several approaches have\\nbeen proposed: Echo Embedding[11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semantics. LLM2Vec[12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dependency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction.\\nAnother widely adopted approach is knowledge distillation,\\nwhere text embeddings are treated as the ”signal states” representing textual seman-\\ntics. By distilling knowledge from high-performing teacher models to student models,\\nthe objective is to optimize the embedding performance. For instance, Jasper[14] em-\\nploys a multi-stage knowledge distillation framework, combining with multiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debater[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, iteratively optimizing doc-\\nument representations through continuous COT. Distillation is applied to constrain\\nthe ﬁnal token representation to learn the optimal semantic states from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for model optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to diminishing gra-\\ndient norms during optimization. Thus they developed an asynchronous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refreshes the negative\\nsample pool using the current model parameters, thereby ensuring the maintenance\\nof up-to-date and optimally challenging negative samples. Both Conan-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering mechanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerful Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ntor representation, we propose a data synthesis method by employing three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building upon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling batch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a two-phase train-\\ning approach, through the ﬁrst-stage retrieval training and second-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilities, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state-of-the-art av-\\nerage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pair classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintaining re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTEB and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed methods.\\n2\\nRelated Works\\n2.1\\nText Embedding Models\\nText vector representation is a fundamental research area in natural language processing\\n(NLP) and serves as the cornerstone for language understanding. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[25], BM25[26], and LSA[27]. With\\nthe advent of pretrained language models, dense contextualized representations based\\non architectures like BERT[2] and T5[3] became widely studied and applied[4][5][6]. In\\nthe era of large language models (LLMs), major advancements have led to the devel-\\nopment of LLM-based embedding models, such as text-embedding-3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—such\\nas RoPE positional encoding[35], RMSNorm[36], and GeGLU activation[37]—combined\\nwith their strong semantic contextualization capabilities acquired through large-scale\\n3'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 3, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior performance in re-\\ntrieval and related tasks.\\n2.2\\nEmbedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrastive learning training on\\nhigh-quality labeled positive and negative samples.\\nIn unsupervised learning, early\\nwork like SimCSE[7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance the model’s dis-\\ncriminative representation capability. For weakly supervised learning, gte[33] utilized\\nlarge-scale structured data (web search data, title-article pairs, etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categories from the per-\\nspective of circle loss[40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3\\nData Synthesis\\nData quantity and quality are the most critical factors in model optimization, data\\nsynthesis methods have become a critical research direction due to the high cost of\\nmanual annotation.\\nDoc2Query[41] and Query2Doc[42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents respectively, enhancing data\\nfor improved RAG performance.\\nPromptagator[43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot demonstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varying intents or distri-\\nbutions.\\nGPL[44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieval models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unnatural Instructions[45]\\nleverages prompt and in-context learning (ICL) techniques to generate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experimental results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-speciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 4, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n2.4\\nHard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive learning for retrieval model\\ntraining. Early work like ANCE[46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint states to maintain\\noptimally challenging samples. Conan-Embedding[24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refreshing samples when\\ntheir scores fall below a threshold. NV-Retriever[47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering criteria to minimize\\nfalse negatives. LGAI-Embedding[17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including retrieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them collectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-wise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[39], SFR-Embedding[30], NV-Embed[47], Conan-Embedding[24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-\\nanisms. However, recent large language models predominantly adopt decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constraining tokens’ ability to\\ncapture contextual information. Several studies have addressed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoint due to its exceptional\\nChinese language contextual capabilities. Consequently, we implemented the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-directional attention\\n5'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 5, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The model architecture is\\nshown in Figure 1\\n3.2\\nData Transformation\\n3.2.1\\nRetrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper and QA datasets.\\nGiven the heterogeneous nature of these datasets across domains and purposes, we\\ndesign a retrieval-oriented data transformation methodology to convert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their processing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transformation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is applied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commonly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets generally contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 6, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted into a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits remarkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question paired with one\\nanswer) represents the most suitable format for retrieval training. For transfor-\\nmation, the ”Question/Query/User” portion is converted into queries, while the\\n”Answer/Response/Assistant” portion is processed as documents.\\n3.2.2\\nNLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment, and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) and textual entailment\\ntasks as illustrative examples. Our approach distinctively reformulates NLI tasks into\\ntext pair-score formats compatible with Cosent loss[49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationships. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric se-\\nmantic matching to determine whether two sentences share equivalent meaning.\\nSTS datasets typically consist of sentence pairs with associated labels, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical scores (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical value of 1, while\\n”no”/”false” are converted to 0. The data is then structured into (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each single original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neutral, contradic-\\ntion.\\nOur processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contradiction respec-\\ntively. We construct (query, document, score) triplets accordingly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3\\nCLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the same category\\nexhibit semantic proximity while distinct boundaries separate diﬀerent classes. NV-\\nEmbed[47] compared label-based and example-based data construction methods, with\\nexperimental results demonstrating the superiority of the latter. Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by using the text as query,\\n7'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 7, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, and selecting texts\\nfrom diﬀerent labels as negative examples.\\nFigure 2 provides a detailed schematic\\nillustration of this process.\\n3.3\\nTraining Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized training objectives to\\nto enhance model training eﬃciency.\\nThis section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1\\nRetrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[48], but incorporate an\\nimprovement inspired by gte[33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a batch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is explicitly described\\nin Equation (1).\\nLRetrieval = −1\\nn\\nX\\ni\\nlog\\nesim(qi,d+\\ni )/τ\\nesim(qi,d+\\ni )/τ + P\\nj esim(qi,d−\\nj )/τ + P\\nj̸=i esim(qi,qj)/τ\\n(1)\\n3.3.2\\nNLI\\nFor NLI tasks, the transformed labels are numerically comparable and exhibit ordinal\\nrelationships.\\nWe employ Cosent loss[49] to optimize such data, which is designed\\nbased on the principles of Circle loss[40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demonstrating faster\\nconvergence. Its mathematical formulation is presented in Equation (2).\\n8'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 8, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nLNLI = log(1 +\\nX\\nsim(i,j)>sim(k,l)\\nexp(sim(xk, xl) −sim(xi, xj)\\nτ\\n))\\n(2)\\n3.3.3\\nCLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However, since CLS data is\\nprocessed in an example-based manner, directly applying in-batch negative sampling\\non classiﬁcation datasets with limited categories may lead to false negatives from items\\nof diﬀerent classes.\\nNumerous studies have proposed diverse approaches to address\\nthis issue[51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling, for each negative\\nsample from other data instances, we check whether its label matches the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remains InfoNCE, with\\nthe CLS loss formulation shown in Equation (3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = −1\\nn\\nX\\ni\\nlog esim(ti,t+\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t+\\ni )/τ +\\nX\\nn\\nMASK(ti, t−\\ni,n) · esim(ti,t−\\ni,n)/τ+\\nX\\nj̸=i\\nMASK(ti, tj) · esim(ti,tj)/τ+\\nX\\nj̸=i\\nX\\nn\\nMASK(ti, t−\\nj,n) · esim(ti,t−\\nj,n)/τ\\nand Cti = Ct+\\ni\\nand MASK(ti, tj) =\\n(\\n0\\nif Cti = Ctj,\\n1\\notherwise\\n4\\nData Synthesis\\nThe production of higher-quality data through data production has gained critical im-\\nportance in embedding training.\\nManual annotation incurs higher costs and lower\\nproduction eﬃciency, thus developing eﬀective automated data synthesis methods has\\nemerged as a key research focus. Recent advancements in large language models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accurate interpretation\\nof human instructions and generation of high-quality outputs. Multiple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[28][34], we similarly\\n9'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 9, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nleverages LLM capabilities for data production across three dimensions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃculty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The constraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1\\nStructural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and grammatical features,\\nwhich represent relatively surface-level characteristics reﬂecting word arrangements,\\ncombinations, tenses, voices, and other formal attributes.\\nEmbedding models must\\naccurately capture underlying semantics despite variations in surface form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantically equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to structural variations\\nwhile accurately capturing semantic information, we propose a Paraphrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented instances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure 3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2\\nSemantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcations yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure uniform vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphrasing, we propose an\\naugmentation method using LLM to diversify semantics. The core concept is: given a\\n10'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 10, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the domain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, and viewpoints while\\nremaining contextually anchored. This process is governed via prompt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3\\nMore challenging embeddings\\nHard negative examples are crucial for enhancing the performance of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, the framework is\\nillustrated in Figure 5.\\nDuring Data paraphrasing and Augmentation, we implement task-speciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs and add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by randomly duplicating\\nexisting entries containing the original sentences and replacing them with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 11, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 6: Training pipeline\\n5\\nTraining Optimization\\n5.1\\nData Grouping Strategy\\nPrior works like Linq-Embedding[52] and SFR-Embedding-Mistral[30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixing them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointers recorded to\\nenable sequential reading in subsequent iterations. For sampling weights, we adopt\\nthe data sampling strategy from gte[33] and mgte[50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation (4)\\npi =\\nlα\\ni\\nPm\\nj=1 lα\\nj\\n(4)\\n5.2\\nTwo-Stage Training\\nInspired by NV-Embed’s[47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusively uses retrieval-\\noriented training data, while the second stage integrates both retrieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Data Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating additional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 12, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nfunction to control the proportion of retrieval training, ensuring that throughout the\\nsecond training stage, the computational contribution of retrieval data accounts for η,\\nwhile non-retrieval data constitutes 1−η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling ratio determination. Let\\nthe training data D = [d1, d2, ..., dN] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [l1, l2, ..., lN]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scaling factor α, a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets for summation.\\nThe equations are as follows:\\nSret =\\nX\\ni\\nMi · lα\\ni\\nSnon ret =\\nX\\ni\\n(1 −Mi) · lα\\ni\\nwhere Mi =\\n(\\n0\\nif di ∈RET,\\n1\\nelse\\nwhere RET denotes the set of retrieval training datasets. The retrieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [lsamp\\n1\\n, lsamp\\n2\\n, ...lsamp\\nN\\n]\\nwhere lsamp\\ni\\n=\\n(ηRET ·lα\\ni\\nSret\\nif di ∈RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6\\nExperiments\\n6.1\\nTraining Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-multilingual-gemma2-\\ndata 3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[28],\\nEcho Embedding[11], and LLM2Vec[12], is also incorporated.\\nThe aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[64], Natural Questions (NQ)[65], ELI5[66], HotpotQA[67],\\nMIRACL[68], SQuAD[69], FEVER[70], Quora Question Pairs(QQP), and DuReader[71],\\netc.\\nPrevious researchers have already systematically collected and organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such as Huatuo medical QA6,\\nall above data has been incorporated. Additional data from huggingface’s sentence-\\ntransformers7 repository includes reddit, hover[72], mr-tydi[73], law-gpt, and s2orc[74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 13, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nOther sources encompass web questions, BioASQ[54], cmrc[55], CSL8, nli for simcse\\n(used in SimCSE[7] and GTE[33]), MLDR9, GLUE Benchmark[56], Yelp Reviews[57]\\nand Weibo Sentiment10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb-Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test sets.\\nFor data requiring format conversion, we apply the methodologies described in Sen-\\ntion 3.2.\\nDatasets with limited samples (e.g., subsets of bge and e5 series, Imdb-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultimately obtained ap-\\nproximately 5M high-quality training samples through API interfaces. We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores using GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic hard negative\\ngeneration. Due to API cost constraints, only 30% of hard negatives are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3-1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal training dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2\\nTrainset Instructions\\nFor most training data containing instruction formats, we retain their original con-\\ntents. For the MTEB training set, we adopt instructions corresponding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external data lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-speciﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix A.2.\\n6.3\\nTraining Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm-up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the second stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k steps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considering data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the absence of forward\\ncomputation for negative samples, the batch size is set to 768. Across all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine temperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Grouping Strategy\\nremains unchanged between the two stages, except that the second stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 14, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all stages to ensure\\nmaximum performance improvement. The query and passage lengths are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoPE[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations for all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem\\nStage1\\nStage2\\nWarm-up\\n300\\nSteps\\n3e-5\\n2e-5\\nLR\\n32k\\n8k\\nBatch Size InfoNCE\\n256\\nBatch Size Cosent\\n-\\n768\\nPrecision\\nbﬂoat16\\nTemperature\\n0.02\\nOptimizer\\nAdam\\nQuery Length\\n256\\nPassage Length\\n1536\\n6.4\\nCompared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MTEB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For MTEB, the compar-\\native models include LGAI-Embedding-Preview[17], the Seed series (v1.5[75] , v1.6[38]),\\nQwen series (8B, 4B)[34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[30],\\nand NV-Embed-v2[47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[39].\\n6.5\\nMain Results\\nThis section presents the evaluation results of Qzhou-embedding on MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranked models. As detailed\\nin Table 2, Table 3, Qzhou-embedding achieves state-of-the-art performance across\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach.\\nFurthermore, under MTEB’s oﬃcial ranking protocol, Qzhou-embedding\\nsecured the top position on both leaderboards. (Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the listed models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/classiﬁcation\\nbenchmark where the top score does not appear in the top 10 models.)\\n15'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 15, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nSumm.\\nMean(Task)\\nMean(TaskType)\\nLGAI-Embedding-Preview\\n89.97\\n59.25\\n88.67\\n49.13\\n66.18\\n86.69\\n38.93\\n74.12\\n68.4\\nSeed1.5-Embedding\\n89.88\\n60.83\\n87.39\\n50.67\\n67.45\\n87.23\\n36.44\\n74.76\\n68.56\\nQwen3-Embedding-8B\\n90.43\\n58.57\\n87.52\\n51.56\\n69.44\\n88.58\\n34.83\\n75.22\\n68.71\\nQwen3-Embedding-4B\\n89.84\\n57.51\\n87.01\\n50.76\\n68.46\\n88.72\\n34.39\\n74.6\\n68.1\\nSeed1.6-embedding\\n92.42\\n59.22\\n85.07\\n50.28\\n64.9\\n86.87\\n37.1\\n74.07\\n67.98\\ngemini-embedding-001\\n90.05\\n59.39\\n87.7\\n48.59\\n64.35\\n85.29\\n38.28\\n73.3\\n67.67\\njasper en vision language v1\\n90.27\\n60.52\\n88.14\\n50\\n56.05\\n84.37\\n37.19\\n71.41\\n66.65\\nLinq-Embed-Mistral\\n83\\n54.07\\n88.44\\n49.44\\n60.14\\n84.69\\n37.26\\n69.8\\n65.29\\nSFR-Embedding-Mistral\\n80.47\\n54.93\\n88.59\\n50.15\\n59.33\\n84.77\\n36.32\\n69.31\\n64.94\\nNV-Embed-v2\\n87.19\\n47.66\\n88.69\\n49.61\\n62.84\\n83.82\\n35.21\\n69.81\\n65\\nQZhou-Embedding(Ours)\\n88.97\\n61.65\\n92.43\\n51.77\\n67.12\\n91.65\\n33.05\\n75.97\\n69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nMean(Task)\\nMean(TaskType)\\nSeed1.6-embedding\\n77.98\\n73.11\\n88.71\\n71.65\\n79.69\\n68.94\\n75.63\\n76.68\\nSeed1.5-Embedding\\n79.37\\n71.11\\n89.57\\n70.14\\n79.33\\n66.56\\n74.87\\n76.01\\nritrieve zh v1\\n76.88\\n66.5\\n85.98\\n72.86\\n76.97\\n63.92\\n72.71\\n73.85\\nConan-embedding-v2\\n76.47\\n68.84\\n92.44\\n74.41\\n78.31\\n65.48\\n74.24\\n75.99\\nxiaobu-embedding-v2\\n76.53\\n65.17\\n85.94\\n72.58\\n76.49\\n64.18\\n72.36\\n73.48\\nQwen3-Embedding-8B\\n76.97\\n80.08\\n84.23\\n66.99\\n78.21\\n63.53\\n73.84\\n75\\nConan-embedding-v1\\n76.77\\n66.33\\n85.68\\n72.76\\n76.67\\n63.67\\n72.5\\n73.65\\nzpoint large embedding zh\\n76.4\\n62.23\\n85.75\\n72.33\\n76.36\\n63.86\\n71.81\\n72.82\\npiccolo-large-zh-v2\\n76.42\\n62.16\\n85.22\\n70\\n74.36\\n63.46\\n70.86\\n71.94\\nQwen3-Embedding-4B\\n75.46\\n77.89\\n83.34\\n66.05\\n77.03\\n61.26\\n72.27\\n73.51\\nQZhou-Embedding(Ours)\\n79.99\\n70.91\\n95.07\\n74.85\\n78.80\\n71.89\\n76.99\\n78.58\\n7\\nConclusion\\nIn this technical report, we present QZhou-Embedding, a general-purpose contextual\\ntext embedding model with exceptional text representation capabilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transformation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we developed a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques such as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a two-stage training\\nstrategy comprising initial retrieval-focused training followed by full-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robust retrieval per-\\nformance.\\nThe model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings establish that data qual-\\nity and diversity are pivotal for improving embedding model capabilities. In the future,\\nwe will focus on developing multimodal and multilingual embedding models, as well\\nas exploring eﬀective applications of embedding models in agent systems, aiming to\\nintegrate cutting-edge technologies to optimize this classical module.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceedings\\n16'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 16, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conference on Research and\\nDevelopment in Information Retrieval, organised by Dublin City University, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine learning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-supervised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert:\\nSentence embeddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conference on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics.\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-shot learners.”\\nAdvances in neural information processing systems 33 (2020): 1877-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”Fine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” arXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large language models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.05961 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 17, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jasper and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv:2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective representa-\\ntions for dense retrieval through deliberate thinking before search.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical report[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405.17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:2407.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu Lian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embeddings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embedding:\\nGen-\\neral text embedding with more and better negative samples.” arXiv preprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–idf measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Conference on Research\\nand Development in Information Retrieval, organised by Dublin City University,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 18, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and\\nFuru Wei. Improving text embeddings with large language models. arXiv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with transfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Representations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learners. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage contrastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embedding.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer normalization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer,\\nNoam.\\n”Glu\\nvariants\\nimprove\\ntransformer.”\\narXiv\\npreprint\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” arXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarity op-\\ntimization.” In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 19, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with\\nlarge language models. In Proceedings of the 2023 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapore. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense\\nretrieval from 8 examples. In The Eleventh International Conference on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, pages\\n2345–2360, Seattle, United States. Association for Computational Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unnatural in-\\nstructions: Tuning language models with (almost) no human labor.” arXiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large language\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevating text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 20, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competition[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chinese machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis\\nplatform for natural language understanding[J]. arXiv preprint arXiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sentiment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association for computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural language understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Proceedings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computational linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAWS-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation.” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine read-\\ning comprehension dataset. In Proceedings of the Workshop on Cognitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-located with the\\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\n21'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 21, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\\net al. Natural questions: a benchmark for question answering research. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019.\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\\nMichael Auli. 2019. ELI5:\\nLong Form Question Answering. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages\\n3558–3567, Florence, Italy. Association for Computational Linguistics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2023.\\n[69] Pranav\\nRajpurkar,\\nJian\\nZhang,\\nKonstantin\\nLopyrev,\\nand\\nPercy\\nLiang.\\nSquad:\\n100,000+ questions for machine comprehension of text. arXiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang.\\n2018. DuReader: a Chinese Machine Reading Comprehension Dataset from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Reading for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association for Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extraction And Claim\\nVeriﬁcation. In Findings of the Association for Computational Linguistics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Linguistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark for dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 22, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar\\nNaim, Gustavo Hernandez ´ Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA\\nAppendix\\nA.1\\nFramework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem\\nExplanation\\nKeep core semantics\\nPreserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within±15%\\nThe length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language\\nThe language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld\\nThe content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 23, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nA.2\\nInstruction Examples\\nTable 5: Instruction for partial training data\\nDataset\\nInstruction\\nHuatuo\\nGiven a medical question, retrieve user replies that\\nbest answer the question\\nReddit\\nRetrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT\\nRetrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI\\nRetrieve semantically similar text\\nYelp\\nClassify the customer review of businesses\\nWeibo\\nClassify the sentiment of Weibo comments\\nA.3\\nData Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, followed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery\\npos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 24, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery\\npos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury.\\nBy the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 25, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 8: Augmentation Example\\nquery\\npos\\nneg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire\\na\\nrefundable\\nde-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards\\nmay\\naccept\\nlower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks\\noﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By\\nthe\\ntime\\nof\\nEliz-\\nabethan literature a vig-\\norous\\nliterary\\nculture\\nin\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The\\nFaerie\\nQueene’,\\nan\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater\\neclipsed\\nby\\nShake-\\nspeare’s popularity.\\nChristopher\\nMarlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork\\nit\\nwasn’t\\nan\\nepic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical\\nthemes\\nthrough\\nblank\\nverse\\nand\\nbecame\\na\\ncornerstone\\nof\\nEnglish\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso\\ndealt\\nwith\\nreligious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 26, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery\\npos\\nneg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli\\nexpansion\\nduring\\nthe\\nArab-Israeli\\nconﬂicts,\\nthough\\nhis\\nwarnings\\nto\\nNasser\\nwere delayed\\nand\\ninitially\\ndismissed,\\nwhile\\nother Arab leaders focused\\nmore\\non\\ndirect\\nmilitary\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand\\nmixed\\nwith\\nbroader\\nregional\\ntensions,\\nwhile\\nEgyptian\\nmilitary\\nmove-\\nments in Sinai were already\\nunderway\\nunder\\nAmer’s\\norders.\\n27')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter = RecursiveCharacterTextSplitter (\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap = chunk_overlap,\n",
        "        length_function = len,\n",
        "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
        "\n",
        "    if split_docs:\n",
        "        print(f\"Example Chunk\")\n",
        "        print(f\"Content: {split_docs[0].page_content[:200]}\")\n",
        "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
        "\n",
        "    return split_docs"
      ],
      "metadata": {
        "id": "1N2mHYVJxgZu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List,Any,Tuple,Dict\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "WF_J0WZXxtf7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks= split_documents(all_pdfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m38AKYqmxm_5",
        "outputId": "cdfa9a88-2896-4aea-dcf8-ef9b553a5a01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 27 documents into 89 chunks\n",
            "Example Chunk\n",
            "Content: QZhou-Embedding Technical Report\n",
            "Kingsoft AI\n",
            "QZhou-Embedding Technical Report\n",
            "Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\n",
            "Kingsoft AI∗\n",
            "August 2025\n",
            "Abstract\n",
            "We present QZhou-Embedding, a general-\n",
            "Metadata: {'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 0, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzefyPeTAR4h",
        "outputId": "96c06bec-cf4b-4c51-c161-1c87c1540fb1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 0, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilities.\\nBuilt upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁed multi-task frame-\\nwork comprising specialized data transformation and training strategies.\\nThe\\ndata transformation scheme enables the incorporation of more diverse textual\\ntraining datasets, while the task-speciﬁc training strategies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline leveraging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentation, and Hard negative\\nexample generation to improve the semantic richness and sample diﬃculty of\\nthe training set. Additionally, we employ a two-stage training strategy, compris-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 0, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sample diﬃculty of\\nthe training set. Additionally, we employ a two-stage training strategy, compris-\\ning initial retrieval-focused pretraining followed by full-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robust retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more diverse data is crucial for\\nadvancing retrieval model performance, and that leveraging LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructions on GitHub2.\\n1\\nIntroduction'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 0, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructions on GitHub2.\\n1\\nIntroduction\\nText embedding models, which transform natural language text into mathematical vec-\\ntor representations, play an indispensable role in text mining, question-answering sys-\\ntems, recommendation systems, and retrieval-augmented generation. Recently, LLM-\\nbased agent technology has experienced rapid development and widespread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhanced agent systems\\n∗https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nin terms of real-time performance, long-term memory, data privacy preservation, and\\nknowledge integration capabilities. With the continuous advancement of neural net-\\nworks and deep learning, text embeddings have evolved from early sparse representa-\\ntions (e.g., BM25[1]) to dense representations based on ﬁne-tuned deep networks such\\nas BERT[2] and T5[3], leading to signiﬁcant performance improvements[4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatGPT[9], ushered in\\na new era of text embeddings based on LLM representations, including models like text-\\nembedding-3-large and RepLLaMA[10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For instance, to address\\nthe limitation of decoder-only architectures—where causal attention mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—several approaches have'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal attention mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—several approaches have\\nbeen proposed: Echo Embedding[11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semantics. LLM2Vec[12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dependency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction.\\nAnother widely adopted approach is knowledge distillation,\\nwhere text embeddings are treated as the ”signal states” representing textual seman-\\ntics. By distilling knowledge from high-performing teacher models to student models,\\nthe objective is to optimize the embedding performance. For instance, Jasper[14] em-\\nploys a multi-stage knowledge distillation framework, combining with multiple carefully'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instance, Jasper[14] em-\\nploys a multi-stage knowledge distillation framework, combining with multiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debater[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, iteratively optimizing doc-\\nument representations through continuous COT. Distillation is applied to constrain\\nthe ﬁnal token representation to learn the optimal semantic states from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for model optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to diminishing gra-\\ndient norms during optimization. Thus they developed an asynchronous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refreshes the negative'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchronous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refreshes the negative\\nsample pool using the current model parameters, thereby ensuring the maintenance\\nof up-to-date and optimally challenging negative samples. Both Conan-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering mechanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerful Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 1, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ntor representation, we propose a data synthesis method by employing three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building upon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling batch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a two-phase train-\\ning approach, through the ﬁrst-stage retrieval training and second-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilities, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state-of-the-art av-\\nerage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pair classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintaining re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTEB and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed methods.\\n2\\nRelated Works\\n2.1\\nText Embedding Models\\nText vector representation is a fundamental research area in natural language processing\\n(NLP) and serves as the cornerstone for language understanding. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[25], BM25[26], and LSA[27]. With\\nthe advent of pretrained language models, dense contextualized representations based\\non architectures like BERT[2] and T5[3] became widely studied and applied[4][5][6]. In'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized representations based\\non architectures like BERT[2] and T5[3] became widely studied and applied[4][5][6]. In\\nthe era of large language models (LLMs), major advancements have led to the devel-\\nopment of LLM-based embedding models, such as text-embedding-3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—such\\nas RoPE positional encoding[35], RMSNorm[36], and GeGLU activation[37]—combined\\nwith their strong semantic contextualization capabilities acquired through large-scale\\n3'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 3, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior performance in re-\\ntrieval and related tasks.\\n2.2\\nEmbedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrastive learning training on\\nhigh-quality labeled positive and negative samples.\\nIn unsupervised learning, early\\nwork like SimCSE[7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance the model’s dis-\\ncriminative representation capability. For weakly supervised learning, gte[33] utilized\\nlarge-scale structured data (web search data, title-article pairs, etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 3, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 3, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='we also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categories from the per-\\nspective of circle loss[40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3\\nData Synthesis\\nData quantity and quality are the most critical factors in model optimization, data\\nsynthesis methods have become a critical research direction due to the high cost of\\nmanual annotation.\\nDoc2Query[41] and Query2Doc[42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents respectively, enhancing data\\nfor improved RAG performance.\\nPromptagator[43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot demonstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varying intents or distri-\\nbutions.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 3, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='narios by generating queries of diverse intents using few-shot demonstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varying intents or distri-\\nbutions.\\nGPL[44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieval models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unnatural Instructions[45]\\nleverages prompt and in-context learning (ICL) techniques to generate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experimental results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-speciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 4, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n2.4\\nHard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive learning for retrieval model\\ntraining. Early work like ANCE[46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint states to maintain\\noptimally challenging samples. Conan-Embedding[24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refreshing samples when\\ntheir scores fall below a threshold. NV-Retriever[47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering criteria to minimize\\nfalse negatives. LGAI-Embedding[17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 4, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including retrieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them collectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-wise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[39], SFR-Embedding[30], NV-Embed[47], Conan-Embedding[24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 4, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 4, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-\\nanisms. However, recent large language models predominantly adopt decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constraining tokens’ ability to\\ncapture contextual information. Several studies have addressed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoint due to its exceptional\\nChinese language contextual capabilities. Consequently, we implemented the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-directional attention\\n5'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 5, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The model architecture is\\nshown in Figure 1\\n3.2\\nData Transformation\\n3.2.1\\nRetrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper and QA datasets.\\nGiven the heterogeneous nature of these datasets across domains and purposes, we\\ndesign a retrieval-oriented data transformation methodology to convert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their processing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 5, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their processing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transformation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is applied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commonly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets generally contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 6, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted into a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits remarkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question paired with one\\nanswer) represents the most suitable format for retrieval training. For transfor-\\nmation, the ”Question/Query/User” portion is converted into queries, while the\\n”Answer/Response/Assistant” portion is processed as documents.\\n3.2.2\\nNLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment, and sentiment anal-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 6, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='3.2.2\\nNLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment, and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) and textual entailment\\ntasks as illustrative examples. Our approach distinctively reformulates NLI tasks into\\ntext pair-score formats compatible with Cosent loss[49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationships. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric se-\\nmantic matching to determine whether two sentences share equivalent meaning.\\nSTS datasets typically consist of sentence pairs with associated labels, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical scores (e.g., 1.2, 3.1,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 6, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='STS datasets typically consist of sentence pairs with associated labels, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical scores (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical value of 1, while\\n”no”/”false” are converted to 0. The data is then structured into (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each single original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neutral, contradic-\\ntion.\\nOur processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contradiction respec-\\ntively. We construct (query, document, score) triplets accordingly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3\\nCLS-oriented Process'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 6, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='tively. We construct (query, document, score) triplets accordingly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3\\nCLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the same category\\nexhibit semantic proximity while distinct boundaries separate diﬀerent classes. NV-\\nEmbed[47] compared label-based and example-based data construction methods, with\\nexperimental results demonstrating the superiority of the latter. Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by using the text as query,\\n7'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 7, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, and selecting texts\\nfrom diﬀerent labels as negative examples.\\nFigure 2 provides a detailed schematic\\nillustration of this process.\\n3.3\\nTraining Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized training objectives to\\nto enhance model training eﬃciency.\\nThis section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1\\nRetrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[48], but incorporate an\\nimprovement inspired by gte[33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a batch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is explicitly described'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 7, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a batch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is explicitly described\\nin Equation (1).\\nLRetrieval = −1\\nn\\nX\\ni\\nlog\\nesim(qi,d+\\ni )/τ\\nesim(qi,d+\\ni )/τ + P\\nj esim(qi,d−\\nj )/τ + P\\nj̸=i esim(qi,qj)/τ\\n(1)\\n3.3.2\\nNLI\\nFor NLI tasks, the transformed labels are numerically comparable and exhibit ordinal\\nrelationships.\\nWe employ Cosent loss[49] to optimize such data, which is designed\\nbased on the principles of Circle loss[40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demonstrating faster\\nconvergence. Its mathematical formulation is presented in Equation (2).\\n8'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 8, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nLNLI = log(1 +\\nX\\nsim(i,j)>sim(k,l)\\nexp(sim(xk, xl) −sim(xi, xj)\\nτ\\n))\\n(2)\\n3.3.3\\nCLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However, since CLS data is\\nprocessed in an example-based manner, directly applying in-batch negative sampling\\non classiﬁcation datasets with limited categories may lead to false negatives from items\\nof diﬀerent classes.\\nNumerous studies have proposed diverse approaches to address\\nthis issue[51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling, for each negative\\nsample from other data instances, we check whether its label matches the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remains InfoNCE, with'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 8, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='class. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remains InfoNCE, with\\nthe CLS loss formulation shown in Equation (3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = −1\\nn\\nX\\ni\\nlog esim(ti,t+\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t+\\ni )/τ +\\nX\\nn\\nMASK(ti, t−\\ni,n) · esim(ti,t−\\ni,n)/τ+\\nX\\nj̸=i\\nMASK(ti, tj) · esim(ti,tj)/τ+\\nX\\nj̸=i\\nX\\nn\\nMASK(ti, t−\\nj,n) · esim(ti,t−\\nj,n)/τ\\nand Cti = Ct+\\ni\\nand MASK(ti, tj) =\\n(\\n0\\nif Cti = Ctj,\\n1\\notherwise\\n4\\nData Synthesis\\nThe production of higher-quality data through data production has gained critical im-\\nportance in embedding training.\\nManual annotation incurs higher costs and lower\\nproduction eﬃciency, thus developing eﬀective automated data synthesis methods has\\nemerged as a key research focus. Recent advancements in large language models (LLMs)'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 8, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='production eﬃciency, thus developing eﬀective automated data synthesis methods has\\nemerged as a key research focus. Recent advancements in large language models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accurate interpretation\\nof human instructions and generation of high-quality outputs. Multiple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[28][34], we similarly\\n9'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 9, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nleverages LLM capabilities for data production across three dimensions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃculty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The constraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1\\nStructural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and grammatical features,\\nwhich represent relatively surface-level characteristics reﬂecting word arrangements,\\ncombinations, tenses, voices, and other formal attributes.\\nEmbedding models must'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 9, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂecting word arrangements,\\ncombinations, tenses, voices, and other formal attributes.\\nEmbedding models must\\naccurately capture underlying semantics despite variations in surface form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantically equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to structural variations\\nwhile accurately capturing semantic information, we propose a Paraphrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented instances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure 3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 9, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure 3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2\\nSemantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcations yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure uniform vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphrasing, we propose an\\naugmentation method using LLM to diversify semantics. The core concept is: given a\\n10'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 10, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the domain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, and viewpoints while\\nremaining contextually anchored. This process is governed via prompt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3\\nMore challenging embeddings\\nHard negative examples are crucial for enhancing the performance of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, the framework is\\nillustrated in Figure 5.\\nDuring Data paraphrasing and Augmentation, we implement task-speciﬁc strategies:'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 10, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='language models can generate examples that are indistinguishable, the framework is\\nillustrated in Figure 5.\\nDuring Data paraphrasing and Augmentation, we implement task-speciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs and add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by randomly duplicating\\nexisting entries containing the original sentences and replacing them with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 11, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 6: Training pipeline\\n5\\nTraining Optimization\\n5.1\\nData Grouping Strategy\\nPrior works like Linq-Embedding[52] and SFR-Embedding-Mistral[30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixing them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointers recorded to\\nenable sequential reading in subsequent iterations. For sampling weights, we adopt'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 11, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='samples from a single dataset are sampled per batch, with ﬁle pointers recorded to\\nenable sequential reading in subsequent iterations. For sampling weights, we adopt\\nthe data sampling strategy from gte[33] and mgte[50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation (4)\\npi =\\nlα\\ni\\nPm\\nj=1 lα\\nj\\n(4)\\n5.2\\nTwo-Stage Training\\nInspired by NV-Embed’s[47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusively uses retrieval-\\noriented training data, while the second stage integrates both retrieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Data Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 11, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='are incorporated: ﬁrst, we integrate the previously described Data Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating additional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 12, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nfunction to control the proportion of retrieval training, ensuring that throughout the\\nsecond training stage, the computational contribution of retrieval data accounts for η,\\nwhile non-retrieval data constitutes 1−η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling ratio determination. Let\\nthe training data D = [d1, d2, ..., dN] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [l1, l2, ..., lN]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scaling factor α, a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets for summation.\\nThe equations are as follows:\\nSret =\\nX\\ni\\nMi · lα\\ni\\nSnon ret =\\nX\\ni\\n(1 −Mi) · lα\\ni\\nwhere Mi =\\n(\\n0\\nif di ∈RET,\\n1\\nelse\\nwhere RET denotes the set of retrieval training datasets. The retrieval ratio is then'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 12, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Sret =\\nX\\ni\\nMi · lα\\ni\\nSnon ret =\\nX\\ni\\n(1 −Mi) · lα\\ni\\nwhere Mi =\\n(\\n0\\nif di ∈RET,\\n1\\nelse\\nwhere RET denotes the set of retrieval training datasets. The retrieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [lsamp\\n1\\n, lsamp\\n2\\n, ...lsamp\\nN\\n]\\nwhere lsamp\\ni\\n=\\n(ηRET ·lα\\ni\\nSret\\nif di ∈RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6\\nExperiments\\n6.1\\nTraining Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-multilingual-gemma2-\\ndata 3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[28],\\nEcho Embedding[11], and LLM2Vec[12], is also incorporated.\\nThe aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[64], Natural Questions (NQ)[65], ELI5[66], HotpotQA[67],\\nMIRACL[68], SQuAD[69], FEVER[70], Quora Question Pairs(QQP), and DuReader[71],\\netc.\\nPrevious researchers have already systematically collected and organized these'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 12, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='MIRACL[68], SQuAD[69], FEVER[70], Quora Question Pairs(QQP), and DuReader[71],\\netc.\\nPrevious researchers have already systematically collected and organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such as Huatuo medical QA6,\\nall above data has been incorporated. Additional data from huggingface’s sentence-\\ntransformers7 repository includes reddit, hover[72], mr-tydi[73], law-gpt, and s2orc[74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 13, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nOther sources encompass web questions, BioASQ[54], cmrc[55], CSL8, nli for simcse\\n(used in SimCSE[7] and GTE[33]), MLDR9, GLUE Benchmark[56], Yelp Reviews[57]\\nand Weibo Sentiment10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb-Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test sets.\\nFor data requiring format conversion, we apply the methodologies described in Sen-\\ntion 3.2.\\nDatasets with limited samples (e.g., subsets of bge and e5 series, Imdb-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultimately obtained ap-\\nproximately 5M high-quality training samples through API interfaces. We deduplicate'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 13, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='(typically applied to datasets with fewer than 60k samples), we ultimately obtained ap-\\nproximately 5M high-quality training samples through API interfaces. We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores using GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic hard negative\\ngeneration. Due to API cost constraints, only 30% of hard negatives are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3-1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal training dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2\\nTrainset Instructions\\nFor most training data containing instruction formats, we retain their original con-\\ntents. For the MTEB training set, we adopt instructions corresponding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external data lacking instruc-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 13, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='tents. For the MTEB training set, we adopt instructions corresponding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external data lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-speciﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix A.2.\\n6.3\\nTraining Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm-up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the second stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k steps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considering data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the absence of forward'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 13, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='using the InfoNCE loss (i.e., retrieval and classiﬁcation), considering data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the absence of forward\\ncomputation for negative samples, the batch size is set to 768. Across all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine temperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Grouping Strategy\\nremains unchanged between the two stages, except that the second stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 14, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all stages to ensure\\nmaximum performance improvement. The query and passage lengths are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoPE[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations for all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem\\nStage1\\nStage2\\nWarm-up\\n300\\nSteps\\n3e-5\\n2e-5\\nLR\\n32k\\n8k\\nBatch Size InfoNCE\\n256\\nBatch Size Cosent\\n-\\n768\\nPrecision\\nbﬂoat16\\nTemperature\\n0.02\\nOptimizer\\nAdam\\nQuery Length\\n256\\nPassage Length\\n1536\\n6.4\\nCompared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MTEB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For MTEB, the compar-\\native models include LGAI-Embedding-Preview[17], the Seed series (v1.5[75] , v1.6[38]),'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 14, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For MTEB, the compar-\\native models include LGAI-Embedding-Preview[17], the Seed series (v1.5[75] , v1.6[38]),\\nQwen series (8B, 4B)[34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[30],\\nand NV-Embed-v2[47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[39].\\n6.5\\nMain Results\\nThis section presents the evaluation results of Qzhou-embedding on MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranked models. As detailed\\nin Table 2, Table 3, Qzhou-embedding achieves state-of-the-art performance across\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach.\\nFurthermore, under MTEB’s oﬃcial ranking protocol, Qzhou-embedding'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 14, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach.\\nFurthermore, under MTEB’s oﬃcial ranking protocol, Qzhou-embedding\\nsecured the top position on both leaderboards. (Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the listed models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/classiﬁcation\\nbenchmark where the top score does not appear in the top 10 models.)\\n15'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 15, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nSumm.\\nMean(Task)\\nMean(TaskType)\\nLGAI-Embedding-Preview\\n89.97\\n59.25\\n88.67\\n49.13\\n66.18\\n86.69\\n38.93\\n74.12\\n68.4\\nSeed1.5-Embedding\\n89.88\\n60.83\\n87.39\\n50.67\\n67.45\\n87.23\\n36.44\\n74.76\\n68.56\\nQwen3-Embedding-8B\\n90.43\\n58.57\\n87.52\\n51.56\\n69.44\\n88.58\\n34.83\\n75.22\\n68.71\\nQwen3-Embedding-4B\\n89.84\\n57.51\\n87.01\\n50.76\\n68.46\\n88.72\\n34.39\\n74.6\\n68.1\\nSeed1.6-embedding\\n92.42\\n59.22\\n85.07\\n50.28\\n64.9\\n86.87\\n37.1\\n74.07\\n67.98\\ngemini-embedding-001\\n90.05\\n59.39\\n87.7\\n48.59\\n64.35\\n85.29\\n38.28\\n73.3\\n67.67\\njasper en vision language v1\\n90.27\\n60.52\\n88.14\\n50\\n56.05\\n84.37\\n37.19\\n71.41\\n66.65\\nLinq-Embed-Mistral\\n83\\n54.07\\n88.44\\n49.44\\n60.14\\n84.69\\n37.26\\n69.8\\n65.29\\nSFR-Embedding-Mistral\\n80.47\\n54.93\\n88.59\\n50.15\\n59.33\\n84.77\\n36.32\\n69.31\\n64.94\\nNV-Embed-v2\\n87.19\\n47.66\\n88.69\\n49.61\\n62.84\\n83.82\\n35.21\\n69.81\\n65\\nQZhou-Embedding(Ours)\\n88.97\\n61.65\\n92.43\\n51.77\\n67.12\\n91.65\\n33.05\\n75.97\\n69.52'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 15, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='80.47\\n54.93\\n88.59\\n50.15\\n59.33\\n84.77\\n36.32\\n69.31\\n64.94\\nNV-Embed-v2\\n87.19\\n47.66\\n88.69\\n49.61\\n62.84\\n83.82\\n35.21\\n69.81\\n65\\nQZhou-Embedding(Ours)\\n88.97\\n61.65\\n92.43\\n51.77\\n67.12\\n91.65\\n33.05\\n75.97\\n69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nMean(Task)\\nMean(TaskType)\\nSeed1.6-embedding\\n77.98\\n73.11\\n88.71\\n71.65\\n79.69\\n68.94\\n75.63\\n76.68\\nSeed1.5-Embedding\\n79.37\\n71.11\\n89.57\\n70.14\\n79.33\\n66.56\\n74.87\\n76.01\\nritrieve zh v1\\n76.88\\n66.5\\n85.98\\n72.86\\n76.97\\n63.92\\n72.71\\n73.85\\nConan-embedding-v2\\n76.47\\n68.84\\n92.44\\n74.41\\n78.31\\n65.48\\n74.24\\n75.99\\nxiaobu-embedding-v2\\n76.53\\n65.17\\n85.94\\n72.58\\n76.49\\n64.18\\n72.36\\n73.48\\nQwen3-Embedding-8B\\n76.97\\n80.08\\n84.23\\n66.99\\n78.21\\n63.53\\n73.84\\n75\\nConan-embedding-v1\\n76.77\\n66.33\\n85.68\\n72.76\\n76.67\\n63.67\\n72.5\\n73.65\\nzpoint large embedding zh\\n76.4\\n62.23\\n85.75\\n72.33\\n76.36\\n63.86\\n71.81\\n72.82\\npiccolo-large-zh-v2\\n76.42\\n62.16\\n85.22\\n70\\n74.36\\n63.46\\n70.86\\n71.94\\nQwen3-Embedding-4B\\n75.46\\n77.89\\n83.34\\n66.05\\n77.03\\n61.26\\n72.27\\n73.51\\nQZhou-Embedding(Ours)\\n79.99'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 15, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='85.75\\n72.33\\n76.36\\n63.86\\n71.81\\n72.82\\npiccolo-large-zh-v2\\n76.42\\n62.16\\n85.22\\n70\\n74.36\\n63.46\\n70.86\\n71.94\\nQwen3-Embedding-4B\\n75.46\\n77.89\\n83.34\\n66.05\\n77.03\\n61.26\\n72.27\\n73.51\\nQZhou-Embedding(Ours)\\n79.99\\n70.91\\n95.07\\n74.85\\n78.80\\n71.89\\n76.99\\n78.58\\n7\\nConclusion\\nIn this technical report, we present QZhou-Embedding, a general-purpose contextual\\ntext embedding model with exceptional text representation capabilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transformation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we developed a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques such as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a two-stage training\\nstrategy comprising initial retrieval-focused training followed by full-task ﬁne-tuning,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 15, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Augmentation, and Hard negative example generation. We employ a two-stage training\\nstrategy comprising initial retrieval-focused training followed by full-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robust retrieval per-\\nformance.\\nThe model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings establish that data qual-\\nity and diversity are pivotal for improving embedding model capabilities. In the future,\\nwe will focus on developing multimodal and multilingual embedding models, as well\\nas exploring eﬀective applications of embedding models in agent systems, aiming to\\nintegrate cutting-edge technologies to optimize this classical module.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceedings\\n16'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 16, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conference on Research and\\nDevelopment in Information Retrieval, organised by Dublin City University, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine learning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-supervised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 16, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='trastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert:\\nSentence embeddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conference on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics.\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 16, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-shot learners.”\\nAdvances in neural information processing systems 33 (2020): 1877-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”Fine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” arXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large language models are'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 16, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='arXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large language models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.05961 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 17, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jasper and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv:2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective representa-\\ntions for dense retrieval through deliberate thinking before search.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical report[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 17, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405.17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:2407.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu Lian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embeddings.” In Proceedings'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 17, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu Lian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embeddings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embedding:\\nGen-\\neral text embedding with more and better negative samples.” arXiv preprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–idf measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceed-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 17, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Conference on Research\\nand Development in Information Retrieval, organised by Dublin City University,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 18, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and\\nFuru Wei. Improving text embeddings with large language models. arXiv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with transfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 18, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Representations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learners. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage contrastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 18, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embedding.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer normalization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer,\\nNoam.\\n”Glu\\nvariants\\nimprove\\ntransformer.”\\narXiv\\npreprint\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” arXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarity op-\\ntimization.” In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 19, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with\\nlarge language models. In Proceedings of the 2023 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapore. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense\\nretrieval from 8 examples. In The Eleventh International Conference on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 19, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, pages\\n2345–2360, Seattle, United States. Association for Computational Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unnatural in-\\nstructions: Tuning language models with (almost) no human labor.” arXiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 19, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large language\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevating text re-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 19, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevating text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 20, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competition[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chinese machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis\\nplatform for natural language understanding[J]. arXiv preprint arXiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sentiment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association for computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 20, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural language understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Proceedings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 20, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computational linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAWS-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation.” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine read-\\ning comprehension dataset. In Proceedings of the Workshop on Cognitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-located with the'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 20, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Cognitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-located with the\\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\n21'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 21, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\\net al. Natural questions: a benchmark for question answering research. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019.\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\\nMichael Auli. 2019. ELI5:\\nLong Form Question Answering. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages\\n3558–3567, Florence, Italy. Association for Computational Linguistics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 21, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2023.\\n[69] Pranav\\nRajpurkar,\\nJian\\nZhang,\\nKonstantin\\nLopyrev,\\nand\\nPercy\\nLiang.\\nSquad:\\n100,000+ questions for machine comprehension of text. arXiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 21, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang.\\n2018. DuReader: a Chinese Machine Reading Comprehension Dataset from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Reading for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association for Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extraction And Claim\\nVeriﬁcation. In Findings of the Association for Computational Linguistics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Linguistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark for dense'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 21, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Linguistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark for dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 22, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar\\nNaim, Gustavo Hernandez ´ Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA\\nAppendix\\nA.1\\nFramework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem\\nExplanation\\nKeep core semantics\\nPreserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within±15%\\nThe length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language\\nThe language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld\\nThe content must remain strictly aligned with the\\ndomain of the given sentence.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 22, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language\\nThe language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld\\nThe content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 22, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 23, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nA.2\\nInstruction Examples\\nTable 5: Instruction for partial training data\\nDataset\\nInstruction\\nHuatuo\\nGiven a medical question, retrieve user replies that\\nbest answer the question\\nReddit\\nRetrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT\\nRetrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI\\nRetrieve semantically similar text\\nYelp\\nClassify the customer review of businesses\\nWeibo\\nClassify the sentiment of Weibo comments\\nA.3\\nData Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, followed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery\\npos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 23, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 24, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery\\npos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 24, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 24, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury.\\nBy the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 25, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 8: Augmentation Example\\nquery\\npos\\nneg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire\\na\\nrefundable\\nde-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards\\nmay\\naccept\\nlower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 25, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='cards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks\\noﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By\\nthe\\ntime\\nof\\nEliz-\\nabethan literature a vig-\\norous\\nliterary\\nculture\\nin\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The\\nFaerie\\nQueene’,\\nan\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater\\neclipsed\\nby\\nShake-\\nspeare’s popularity.\\nChristopher\\nMarlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork\\nit\\nwasn’t\\nan\\nepic\\nallegory.'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 25, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='later\\neclipsed\\nby\\nShake-\\nspeare’s popularity.\\nChristopher\\nMarlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork\\nit\\nwasn’t\\nan\\nepic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical\\nthemes\\nthrough\\nblank\\nverse\\nand\\nbecame\\na\\ncornerstone\\nof\\nEnglish\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso\\ndealt\\nwith\\nreligious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 26, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery\\npos\\nneg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli\\nexpansion\\nduring\\nthe\\nArab-Israeli\\nconﬂicts,\\nthough\\nhis\\nwarnings\\nto\\nNasser\\nwere delayed\\nand\\ninitially\\ndismissed,\\nwhile\\nother Arab leaders focused\\nmore\\non\\ndirect\\nmilitary\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250916165023', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'page': 26, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf'}, page_content='wary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand\\nmixed\\nwith\\nbroader\\nregional\\ntensions,\\nwhile\\nEgyptian\\nmilitary\\nmove-\\nments in Sinai were already\\nunderway\\nunder\\nAmer’s\\norders.\\n27')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name: str= \"all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        try:\n",
        "            print(f\"Printing the Embedding model: {self.model_name}\")\n",
        "            self.model = SentenceTransformer(self.model_name)\n",
        "            print(f\"Model Loaded Successfully: {self.model.get_sentence_embedding_dimension()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error Loading Model {self.model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "\n",
        "        print(f\"Generatig embeddings for {len(texts)} texts...\")\n",
        "        embeddings = self.model.encode(texts, show_progress=True)\n",
        "        print(f\"Generated Embeddings with shape: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "embeddings_manager = EmbeddingManager()\n",
        "\n",
        "embeddings_manager\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9m9ycVqx51Y",
        "outputId": "e48a9033-0dea-4af2-a638-2bfc738b8ad4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing the Embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded Successfully: 384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EmbeddingManager at 0x7efbef763f50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "  def __init__(self, collection_name: str = \"pdf_documents\", persistent_directory : str = \"../content/sample_data/vector_store\"):\n",
        "    self.collection_name=collection_name\n",
        "    self.persistent_directory=persistent_directory\n",
        "    self.client=None\n",
        "    self.collection=None\n",
        "    self._initialize_store()\n",
        "\n",
        "  def _initialize_store(self):\n",
        "    try:\n",
        "      os.makedirs(self.persistent_directory,exist_ok=True)\n",
        "      self.client=chromadb.PersistentClient(path=self.persistent_directory)\n",
        "      self.collection=self.client.get_or_create_collection(\n",
        "          name=self.collection_name,\n",
        "          metadata={\"description\":\"PDF files embedding for RAG\"}\n",
        "          )\n",
        "      print(f\"Vector Store Initialized. Collection name:{self.collection_name}\")\n",
        "      print(f\"Existing documents in the colection:{self.collection.count()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error initializing vector store: {e}\")\n",
        "      raise\n",
        "  def add_documents(self,documents: List[Any], embeddings: np.ndarray):\n",
        "\n",
        "    if len(documents) != len(embeddings):\n",
        "      raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "\n",
        "    ids=[]\n",
        "    metadatas=[]\n",
        "    documents_text=[]\n",
        "    embeddings_list=[]\n",
        "\n",
        "\n",
        "    for i, (doc,embedding) in enumerate(zip(documents,embeddings)):\n",
        "      doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "      ids.append(doc_id)\n",
        "      metadata=dict(doc.metadata)\n",
        "      metadata[\"doc_index\"]=i\n",
        "      metadata[\"context_length\"]=len(doc.page_content)\n",
        "      metadatas.append(metadata)\n",
        "      documents_text.append(doc.page_content)\n",
        "      embeddings_list.append(embedding.tolist()) # Corrected line: append individual embedding\n",
        "\n",
        "    try:\n",
        "\n",
        "      self.collection.add(\n",
        "          ids=ids,\n",
        "          documents=documents_text,\n",
        "          metadatas=metadatas,\n",
        "          embeddings=embeddings_list\n",
        "      )\n",
        "      print(f\"Added {len(documents)} documents to the vector store\")\n",
        "      print(f\"Total documents in the collection:{self.collection.count()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error adding documents to the vector store: {e}\")\n",
        "      raise\n",
        "\n",
        "vectorStore = VectorStore()\n",
        "\n",
        "vectorStore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7mJWPeUysJ2",
        "outputId": "6e88a6d2-1c1b-412d-8532-4268c073b313"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Store Initialized. Collection name:pdf_documents\n",
            "Existing documents in the colection:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.VectorStore at 0x7efbecc64ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts=[doc.page_content for doc in chunks]\n",
        "\n",
        "embeddings=embeddings_manager.generate_embeddings(texts)\n",
        "\n",
        "vectorStore.add_documents(chunks,embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_kwtPRhURFM",
        "outputId": "e9d5990f-2bbe-4880-9d6f-2739132d784c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generatig embeddings for 89 texts...\n",
            "Generated Embeddings with shape: (89, 384)\n",
            "Added 89 documents to the vector store\n",
            "Total documents in the collection:89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGRetriver:\n",
        "  \"\"\" Handles Query based retrival from Vector Store\"\"\"\n",
        "  def __init__(self,vector_store : VectorStore, embeddings_manager: EmbeddingManager):\n",
        "    self.vector_store=vector_store\n",
        "    self.embeddings_manager=embeddings_manager\n",
        "\n",
        "  def retrieve(self, query: str, top_k: int = 5, threshold: float=0.0) -> List[Dict[str, Any]]:\n",
        "\n",
        "    print(f\"Retrieving documents for query: {query}\")\n",
        "    print(f\"Recieved data for top {top_k} documents with threshold {threshold}\")\n",
        "\n",
        "    query_embedding=self.embeddings_manager.generate_embeddings([query])[0]\n",
        "    #print(query_embedding)\n",
        "\n",
        "    try:\n",
        "      results = self.vector_store.collection.query(\n",
        "          query_embeddings=[query_embedding.tolist()],\n",
        "          n_results=top_k,\n",
        "      )\n",
        "      print(results)\n",
        "      print(f\"Retrieved {len(results['documents'][0])} documents from the vector store\")\n",
        "      retrieved_docs=[]\n",
        "\n",
        "      if results[\"documents\"] and results[\"documents\"][0]:\n",
        "        documents=results[\"documents\"][0]\n",
        "        metadatas=results[\"metadatas\"][0]\n",
        "        distances=results[\"distances\"][0]\n",
        "        ids = results[\"ids\"][0]\n",
        "\n",
        "        for i,(doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
        "\n",
        "          similarity_score = 1-distance\n",
        "          if similarity_score>= threshold:\n",
        "            retrieved_docs.append(\n",
        "                {\n",
        "                    \"id\":doc_id,\n",
        "                    \"content\":document,\n",
        "                    \"metadata\":metadata,\n",
        "                    \"similarity_score\":similarity_score,\n",
        "                    \"distance\":distance,\n",
        "                    \"rank\": i+1\n",
        "                }\n",
        "            )\n",
        "        print(f\"Retrieved {len(retrieved_docs)} documents from the vector store\")\n",
        "      else:\n",
        "        print(f\"No documnets\")\n",
        "      return retrieved_docs\n",
        "    except Exception as e:\n",
        "      print(f\"Error retrieving documents from the vector store: {e}\")\n",
        "      return []\n",
        "\n",
        "retriver=RAGRetriver(vectorStore,embeddings_manager)\n",
        "\n",
        "retriver\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLTNMHszAVWM",
        "outputId": "ff8ff64c-bf12-4ba4-a311-88ecf30e17d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RAGRetriver at 0x7efbe099e000>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriver.retrieve(\"Unified Multi-task Learning Framework\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_HtwvB9MYyK",
        "outputId": "adb7aaac-c418-4da5-8178-b46fc3e3743c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: Unified Multi-task Learning Framework\n",
            "Recieved data for top 5 documents with threshold 0.0\n",
            "Generatig embeddings for 1 texts...\n",
            "Generated Embeddings with shape: (1, 384)\n",
            "{'ids': [['doc_d7b9dd75_9', 'doc_cc25641d_7', 'doc_84161022_18', 'doc_f46ad8ed_13', 'doc_dd87fc60_17']], 'embeddings': None, 'documents': [['erage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task', 'enhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2', 'and Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-', 'followed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-', 'to identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including retrieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them collectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-wise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[39], SFR-Embedding[30], NV-Embed[47], Conan-Embedding[24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'keywords': '', 'creationdate': 'D:20250916165023', 'producer': 'PDFium', 'trapped': '', 'title': '', 'context_length': 931, 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'modDate': '', 'creationDate': 'D:20250916165023', 'doc_index': 9, 'subject': '', 'moddate': '', 'creator': 'PDFium', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'total_pages': 27, 'file_type': 'pdf', 'author': '', 'format': 'PDF 1.7', 'page': 2, 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf'}, {'page': 1, 'context_length': 762, 'modDate': '', 'doc_index': 7, 'producer': 'PDFium', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'moddate': '', 'subject': '', 'creationDate': 'D:20250916165023', 'format': 'PDF 1.7', 'author': '', 'creationdate': 'D:20250916165023', 'trapped': '', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'creator': 'PDFium', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf', 'total_pages': 27, 'keywords': '', 'title': ''}, {'format': 'PDF 1.7', 'keywords': '', 'context_length': 927, 'modDate': '', 'creationDate': 'D:20250916165023', 'moddate': '', 'creationdate': 'D:20250916165023', 'producer': 'PDFium', 'title': '', 'file_type': 'pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'creator': 'PDFium', 'total_pages': 27, 'subject': '', 'page': 4, 'trapped': '', 'author': '', 'doc_index': 18}, {'doc_index': 13, 'subject': '', 'context_length': 948, 'keywords': '', 'modDate': '', 'creationDate': 'D:20250916165023', 'creationdate': 'D:20250916165023', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_type': 'pdf', 'total_pages': 27, 'moddate': '', 'format': 'PDF 1.7', 'producer': 'PDFium', 'page': 3, 'creator': 'PDFium', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'trapped': '', 'title': '', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'author': ''}, {'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'doc_index': 17, 'format': 'PDF 1.7', 'author': '', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'title': '', 'page': 4, 'creationdate': 'D:20250916165023', 'keywords': '', 'file_type': 'pdf', 'producer': 'PDFium', 'creationDate': 'D:20250916165023', 'trapped': '', 'context_length': 984, 'creator': 'PDFium', 'total_pages': 27, 'subject': '', 'moddate': '', 'modDate': ''}]], 'distances': [[0.8568795323371887, 0.8804025053977966, 0.9354847073554993, 0.9977284669876099, 1.0498086214065552]]}\n",
            "Retrieved 5 documents from the vector store\n",
            "Retrieved 4 documents from the vector store\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'doc_d7b9dd75_9',\n",
              "  'content': 'erage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task',\n",
              "  'metadata': {'keywords': '',\n",
              "   'creationdate': 'D:20250916165023',\n",
              "   'producer': 'PDFium',\n",
              "   'trapped': '',\n",
              "   'title': '',\n",
              "   'context_length': 931,\n",
              "   'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'modDate': '',\n",
              "   'creationDate': 'D:20250916165023',\n",
              "   'doc_index': 9,\n",
              "   'subject': '',\n",
              "   'moddate': '',\n",
              "   'creator': 'PDFium',\n",
              "   'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'total_pages': 27,\n",
              "   'file_type': 'pdf',\n",
              "   'author': '',\n",
              "   'format': 'PDF 1.7',\n",
              "   'page': 2,\n",
              "   'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf'},\n",
              "  'similarity_score': 0.14312046766281128,\n",
              "  'distance': 0.8568795323371887,\n",
              "  'rank': 1},\n",
              " {'id': 'doc_cc25641d_7',\n",
              "  'content': 'enhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2',\n",
              "  'metadata': {'page': 1,\n",
              "   'context_length': 762,\n",
              "   'modDate': '',\n",
              "   'doc_index': 7,\n",
              "   'producer': 'PDFium',\n",
              "   'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'moddate': '',\n",
              "   'subject': '',\n",
              "   'creationDate': 'D:20250916165023',\n",
              "   'format': 'PDF 1.7',\n",
              "   'author': '',\n",
              "   'creationdate': 'D:20250916165023',\n",
              "   'trapped': '',\n",
              "   'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'creator': 'PDFium',\n",
              "   'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'file_type': 'pdf',\n",
              "   'total_pages': 27,\n",
              "   'keywords': '',\n",
              "   'title': ''},\n",
              "  'similarity_score': 0.11959749460220337,\n",
              "  'distance': 0.8804025053977966,\n",
              "  'rank': 2},\n",
              " {'id': 'doc_84161022_18',\n",
              "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-',\n",
              "  'metadata': {'format': 'PDF 1.7',\n",
              "   'keywords': '',\n",
              "   'context_length': 927,\n",
              "   'modDate': '',\n",
              "   'creationDate': 'D:20250916165023',\n",
              "   'moddate': '',\n",
              "   'creationdate': 'D:20250916165023',\n",
              "   'producer': 'PDFium',\n",
              "   'title': '',\n",
              "   'file_type': 'pdf',\n",
              "   'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'creator': 'PDFium',\n",
              "   'total_pages': 27,\n",
              "   'subject': '',\n",
              "   'page': 4,\n",
              "   'trapped': '',\n",
              "   'author': '',\n",
              "   'doc_index': 18},\n",
              "  'similarity_score': 0.06451529264450073,\n",
              "  'distance': 0.9354847073554993,\n",
              "  'rank': 3},\n",
              " {'id': 'doc_f46ad8ed_13',\n",
              "  'content': 'followed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-',\n",
              "  'metadata': {'doc_index': 13,\n",
              "   'subject': '',\n",
              "   'context_length': 948,\n",
              "   'keywords': '',\n",
              "   'modDate': '',\n",
              "   'creationDate': 'D:20250916165023',\n",
              "   'creationdate': 'D:20250916165023',\n",
              "   'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'file_type': 'pdf',\n",
              "   'total_pages': 27,\n",
              "   'moddate': '',\n",
              "   'format': 'PDF 1.7',\n",
              "   'producer': 'PDFium',\n",
              "   'page': 3,\n",
              "   'creator': 'PDFium',\n",
              "   'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'trapped': '',\n",
              "   'title': '',\n",
              "   'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf',\n",
              "   'author': ''},\n",
              "  'similarity_score': 0.0022715330123901367,\n",
              "  'distance': 0.9977284669876099,\n",
              "  'rank': 4}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import Cohere\n",
        "import os\n",
        "os.environ[\"COHERE_API_KEY\"] = \"ShfsREOcnyfmLnYtkvOMRCeczeVXqCRQzX1AFWBw\"\n",
        "FM_Model_3 = 'command-light-nightly'\n",
        "llm=Cohere(model = FM_Model_3, max_tokens = 1000, temperature = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBtY5vSd08zr",
        "outputId": "f82b3d6d-e076-4cf2-ed8a-2aeb44b9c1f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2421569965.py:5: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  llm=Cohere(model = FM_Model_3, max_tokens = 1000, temperature = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access your API key from Colab secrets\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_API_KEY') # Please replace with your actual API key\n",
        "\n",
        "# The model 'command-light-nightly' was not found. Please check the Cohere documentation\n",
        "# for available models and ensure your API key has access to the model you want to use.\n",
        "# Replacing with 'command-r' as a common alternative.\n",
        "FM_Model_3 = 'command-a-03-2025' # Replace with an available model from Cohere documentation\n",
        "llm=ChatCohere(model = FM_Model_3, max_tokens = 1000, temperature = 0.1)\n",
        "\n",
        "def simple_RAG(query,rag_retriver,llm,top_k=3):\n",
        "  retrieved_docs=rag_retriver.retrieve(query,top_k=top_k)\n",
        "  context=\"\\n\".join([f\"{doc['content']}\" for doc in retrieved_docs])\n",
        "\n",
        "  if not context.strip():\n",
        "    return \"No Relevant Documents Found\"\n",
        "\n",
        "  prompt =f\"\"\" Use the following context to answer the questio concisely.\n",
        "            context: {context}\n",
        "            Question:{query}\n",
        "            Answer:\"\"\"\n",
        "\n",
        "  response=llm.invoke([prompt.format(context=context,query=query)])\n",
        "  return response.content\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R7MlYY3D1fmb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = simple_RAG(\"What is Unified Multi-task Learning Framework?\",retriver,llm)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iwv3rTYR6tXV",
        "outputId": "76af9df1-a9c0-411d-88ac-29717168e93b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving documents for query: What is Unified Multi-task Learning Framework?\n",
            "Recieved data for top 3 documents with threshold 0.0\n",
            "Generatig embeddings for 1 texts...\n",
            "Generated Embeddings with shape: (1, 384)\n",
            "{'ids': [['doc_d7b9dd75_9', 'doc_cc25641d_7', 'doc_84161022_18']], 'embeddings': None, 'documents': [['erage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task', 'enhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2', 'and Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'author': '', 'modDate': '', 'total_pages': 27, 'subject': '', 'title': '', 'file_type': 'pdf', 'creationDate': 'D:20250916165023', 'context_length': 931, 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'doc_index': 9, 'page': 2, 'creator': 'PDFium', 'producer': 'PDFium', 'format': 'PDF 1.7', 'moddate': '', 'keywords': '', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'creationdate': 'D:20250916165023', 'trapped': ''}, {'moddate': '', 'file_type': 'pdf', 'page': 1, 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'subject': '', 'doc_index': 7, 'creationDate': 'D:20250916165023', 'author': '', 'creator': 'PDFium', 'context_length': 762, 'keywords': '', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'modDate': '', 'producer': 'PDFium', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'trapped': '', 'title': '', 'creationdate': 'D:20250916165023', 'total_pages': 27, 'format': 'PDF 1.7'}, {'total_pages': 27, 'format': 'PDF 1.7', 'author': '', 'creationdate': 'D:20250916165023', 'source_file': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'file_path': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'subject': '', 'source': '../content/sample_data/pdf_files/QZhou-Embedding Technical Report.pdf', 'producer': 'PDFium', 'creationDate': 'D:20250916165023', 'context_length': 927, 'file_type': 'pdf', 'trapped': '', 'doc_index': 18, 'modDate': '', 'page': 4, 'keywords': '', 'creator': 'PDFium', 'title': '', 'moddate': ''}]], 'distances': [[0.9311226606369019, 0.9421032667160034, 0.9693860411643982]]}\n",
            "Retrieved 3 documents from the vector store\n",
            "Retrieved 3 documents from the vector store\n",
            "The Unified Multi-task Learning Framework is a proposed approach that systematically coordinates data processing and training pipelines across multiple tasks, specifically retrieval, natural language inference (NLI), and classification. It enhances dataset diversity and model training efficiency by:\n",
            "\n",
            "1. **Data Transformation**: Adapting data formats to meet the specific requirements of each task, enabling effective feature extraction from heterogeneous sources.  \n",
            "2. **Training Strategy**: Employing specialized loss functions tailored to each task’s characteristics, optimizing training efficiency.  \n",
            "\n",
            "This framework categorizes training data into three task types (retrieval, NLI, and classification) and provides customized solutions for each, allowing diverse natural text data to be converted into embedding training data. It aims to improve embedding capabilities by accommodating cross-domain and cross-task data.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}